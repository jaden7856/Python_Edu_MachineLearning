{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,learning_curve, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LassoCV,ElasticNetCV,Lasso,ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "\n",
    "# 아래 추가적인 패키지가 설치가 되어 있어야 함!!!!!!!\n",
    "from xgboost import XGBRegressor\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "\n",
    "# 시간\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 평가 및 kfold 사용 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=6, random_state= 0, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y_df, \n",
    "                                   scoring=\"neg_mean_squared_error\", cv = kfold))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rigid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ref ) https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KR = KernelRidge()\n",
    "# 기본적으로 해당하지 않는 설절들에 대해서는 KernelRidge에서 알아서 패스하고 안 하는 부분이 있어서 에러에 크게 염려하지 않아도 도 됨!!!\n",
    "KR_param_grid = {\n",
    "    # alpha : 기보적인 모든 모델에 대한 variance를 줄이기 위한 값들.. 기본 1\n",
    "    # 너무 오래 걸려서 좀 간단히 줄여서...\n",
    "    #'alpha' : [0.0001, 0.0005, 0.0008, 0.001, 0.005, 0.009, 0.01, 0.03, 0.05, 0.07, 0.09, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 2, 3, 4],\n",
    "    'alpha' : [0.01, 0.1, 0.5, 1, 1.5, 2 ],\n",
    "    # 사용할 커널의 종류들 : 기본은 linear  이고, 여러가지가 존재함!!!\n",
    "    # 'kernel' : ['polynomial',\"linear\",\"rbf\"],\n",
    "    'kernel' : ['polynomial',\"rbf\"],\n",
    "    # 위의 커널에서  RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels 등을 사용할 때 자동 선택된다.\n",
    "    'gamma':np.logspace(-15, 4, num = 5, base = 2.0),\n",
    "    # 커널에서 Poly에 대한 것을 설정할 때 사용된다!!!\n",
    "    'degree': [2,3],\n",
    "    # 커널 : polynomial and sigmoid kernels 을 사용할 때, 상수항에 대한 계수...\n",
    "    'coef0': [0.5, 1.0, 1.5, 2.0]\n",
    "}\n",
    "\n",
    "t1 = time.time()\n",
    "KR_CV = GridSearchCV(KR, \n",
    "                     param_grid = KR_param_grid, \n",
    "                     cv = kfold, \n",
    "                     scoring = \"neg_mean_squared_error\",\n",
    "                     n_jobs = -1, \n",
    "                     verbose = 1)\n",
    "KR_CV.fit(X_train, y_df)\n",
    "t2 = time.time()\n",
    "print(\"Run Time : \", str(t2-t1))\n",
    "KR_best = KR_CV.best_estimator_\n",
    "print(KR_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Ridge mean score: 0.11196564169797985\n",
      "Kernel Ridge std: 0.007929655583876188\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(KR_best)\n",
    "print(\"Kernel Ridge mean score:\", score.mean())\n",
    "print(\"Kernel Ridge std:\", score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위의 부분을 너무 많이 설정을 하게 되면, 좀 자세히 찾을 수 있겠지만, 너무 시간이 오래 걸리는 부분들이 발생을 한다. 그래서 아래와 같이 randomSearchCV를 사용해보려고 함!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Ridge mean score: 0.11220443541311469\n",
      "Kernel Ridge std: 0.007863732026258856\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(KR_best_RS)\n",
    "print(\"Kernel Ridge mean score:\", score.mean())\n",
    "print(\"Kernel Ridge std:\", score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 25.58 seconds for 20 candidates parameter settings.\n",
      "KernelRidge(alpha=1.5, coef0=2.0, degree=3, gamma=0.0008211879055212056,\n",
      "            kernel='polynomial', kernel_params=None)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "t1 = time.time()\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(KR, param_distributions=KR_param_grid,\n",
    "                                   n_iter=n_iter_search, cv=kfold, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "start = time.time()\n",
    "KR_RS = random_search.fit(X_train, y_df)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time.time() - start), n_iter_search))\n",
    "\n",
    "KR_best_RS = KR_RS.best_estimator_\n",
    "print(KR_best_RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고 ) np.explm1 : exp(x) - 1 ---> 다시 복기하는 부분..\n",
    "y_submission_1 = np.expm1(KR_best_RS.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV,ElasticNetCV,Lasso,ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ref) https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 40 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time : 9.697360038757324\n",
      "Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:    9.6s finished\n"
     ]
    }
   ],
   "source": [
    "Las = Lasso()\n",
    "\n",
    "Las_param_grid = {\n",
    "    # alpha : 기보적인 모든 모델에 대한 variance를 줄이기 위한 값들.. 기본 1, 0일 때는 기존의 ord와 동일!!!\n",
    "    'alpha' : [0.0001, 0.0005, 0.0008, 0.001, 0.005, 0.009, 0.01, 0.03, 0.05, 0.07, 0.09, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 2, 3, 4],\n",
    "    # 모델을 할 때 상수항을 할지 말지 : 기본은 안 한다\n",
    "    'fit_intercept' : [True, False],\n",
    "    # 정규화에 대한 부분 : True를 하면 자동으로 정규화를 L2로 수행을 하고, 아니면 그 전에 StanderScaler 사용\n",
    "    'normalize' : [False, True],\n",
    "}\n",
    "\n",
    "t1 = time.time()\n",
    "Las_CV = GridSearchCV(Las, \n",
    "                     param_grid = Las_param_grid, \n",
    "                     cv = kfold, \n",
    "                     scoring = \"neg_mean_squared_error\",\n",
    "                     n_jobs = -1, \n",
    "                     verbose = 1)\n",
    "Las_CV.fit(X_train, y_df)\n",
    "t2= time.time()\n",
    "print(\"Run Time :\", str(t2-t1))\n",
    "Las_best = Las_CV.best_estimator_\n",
    "print(Las_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso mean score: 0.11250935655377242\n",
      "Lasso std: 0.00903330132980516\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso mean score:\", rmsle_cv(Las_best).mean())\n",
    "print(\"Lasso std:\", rmsle_cv(Las_best).std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV,ElasticNetCV,Lasso,ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 576 candidates, totalling 3456 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   41.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1232 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3232 tasks      | elapsed:  1.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time : 101.20419096946716\n",
      "ElasticNet(alpha=0.01, copy_X=True, fit_intercept=True, l1_ratio=0.2,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=True,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 3456 out of 3456 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "source": [
    "elnet = ElasticNet()\n",
    "\n",
    "elnet_param_grid = {\n",
    "    # 기존 : 위에서 한 alpha의 기능 동일!!!\n",
    "    'alpha' : [0.01, 0.1, 0.5, 1, 1.5, 2 ],\n",
    "    # ElasticNet mixing parameter, with 0 <= l1_ratio <= 1\n",
    "    # l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty.\n",
    "    'l1_ratio' : [0.01, 0.1, 0.2, 0.5, 0.7, 0.8],\n",
    "    # 기존 : 모델을 할 때 상수항을 할지 말지 : 기본은 안 한다\n",
    "    'fit_intercept': [True, False], \n",
    "    # 기존 : 정규화에 대한 부분 : True를 하면 자동으로 정규화를 L2로 수행을 하고, 아니면 그 전에 StanderScaler 사용\n",
    "    'normalize' : [False, True],\n",
    "    # Whether to use a precomputed Gram matrix to speed up calculations\n",
    "    \"precompute\" : [True, False],\n",
    "    # The maximum number of iterations\n",
    "    \"max_iter\" : [1000, 3000],\n",
    "    # The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.\n",
    "    \"tol\" : [0.0001, 0.00001]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "elnet_GS = GridSearchCV(elnet, \n",
    "                     param_grid = elnet_param_grid, \n",
    "                     cv = kfold, \n",
    "                     scoring = \"neg_mean_squared_error\",\n",
    "                     n_jobs = -1, \n",
    "                     verbose = 1)\n",
    "elnet_GS.fit(X_train, y_df)\n",
    "t2= time.time()\n",
    "print(\"Run Time :\", str(t2-t1))\n",
    "elnet_GS_best = elnet_GS.best_estimator_\n",
    "print(elnet_GS_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet mean score: 0.11166291672292976\n",
      "ElasticNet std: 0.009169557729243012\n"
     ]
    }
   ],
   "source": [
    "print(\"ElasticNet mean score:\", rmsle_cv(elnet_GS_best).mean())\n",
    "print(\"ElasticNet std:\", rmsle_cv(elnet_GS_best).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submission_3 = np.expm1(elnet_GS_best.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XG Boost¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ref) https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters   \n",
    "\n",
    "* 파라미터 : 일반 파라미터 / 부스트 파라미터 / 학습과정 파라미터/ 커맨드 라인 파리미터     \n",
    "** 일반 파라미터   \n",
    "1) booster : 어떤 부스팅 방식을 사용할 지 결정 - gbtree, gblinear, dart etc   \n",
    "2) nthread : 몇 개의 thread를 사용해서 동시에 처리할지 - 기본은 최대한 많이 --> 특별히 선택하지 않아도 됨!   \n",
    "3) num_feature  : 특징의 차원의 숫자를 정해야 하는 경우 옵션을 통해서 처리 - 기본은 최대한 많이 이기에, 특별하게 선택하지 않아도 될 듯   \n",
    "** 부스팅 파라미터   \n",
    "1) eta : 러닝 레이트임. 트리에 가지가 많을 수록 overfitting이 발생하기에, 매번 부스팅 스탭마다 weight를 주어서 부스팅 과정에서 과접합이 일어나지 않도록 함.   \n",
    "2) gamma : 정보 획득에서 -r로 펴현이 되는 부분이 있는 감마임. 이 값이 커지게 되면 ㅡㅌ리의 깊이가 줄어들어서 좀 더 보수적인 모델이 된다. 기본값은 0임.  \n",
    "3) max_depth : 한 트리의 최대 깊이를 지정하는 것. 숫자가 커지면 모델의 복잡도가 올라가고, 당연히 overfitting의 위험이 발생하게 된다. 기본은 6으로 되어 있어서 최대는 2^6=64개가 된다.  \n",
    "4) lambda(L2 reg-form) : L2 정규화에 달리는 weight임. 숫자가 클 수록 보수적인 모델임.\n",
    "5) alpha(L1 reg-form) : L1 정규화에 달리는 weight임. 숫자가 클 수록 보수적인 모델이 됨. 그리고 L2보다 아웃라이어에 좀 더 민감하게 작용을 하는 특징이 있음!!!   \n",
    "** 학습 과정 파라미터   \n",
    "1) objective : 목적 함수 (reg:linear, binary:logistic, count:possion  etc)   \n",
    "2) eval_metric : 모델의 평가 함수를 조정하는 함수 . rmse( root mean square error). logloss(log-likelihood), map(mean average precision) etc   \n",
    "** 커맨드 라인 파라미터   \n",
    "1) num_rounds : boosting 라운드를 결정한다. 랜덤하게 하게 되니 적당히 크게 되어야 함. epoch와 동일.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   35.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   35.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.7, 'gamma': 0, 'learning_rate': 0.04, 'max_depth': 3, 'min_child_weight': 0.2, 'n_estimators': 870, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "XGB = XGBRegressor(objective ='reg:squarederror')\n",
    "#XGB = XGBRegressor()\n",
    "\n",
    "xg_param_grid = {\n",
    "              'n_estimators' :[870],\n",
    "              'learning_rate': [0.04],\n",
    "              \n",
    "              'max_depth': [3],\n",
    "              'min_child_weight':[0.2],\n",
    "              \n",
    "              'gamma': [0],\n",
    "                \n",
    "              'subsample':[0.8],\n",
    "              'colsample_bytree':[0.7]\n",
    "    \n",
    "              #'reg_alpha':[0.08,0.09,0.095,0.1,0.15,0.2],\n",
    "              #'reg_lambda':[0,0.001,0.002]\n",
    "              }\n",
    "                \n",
    "gsXGB = GridSearchCV(XGB,param_grid = xg_param_grid, cv=kfold, scoring=\"neg_mean_squared_error\", n_jobs= -1, verbose = 1)\n",
    "gsXGB.fit(X_train,y_df)\n",
    "XGB_best = gsXGB.best_estimator_\n",
    "print(gsXGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XG Boost mean score: 0.1136928510161249\n",
      "XG Boost std: 0.00882904786198643\n"
     ]
    }
   ],
   "source": [
    "print(\"XG Boost mean score:\", rmsle_cv(XGB_best).mean())\n",
    "print(\"XG Boost std:\", rmsle_cv(XGB_best).std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submission_6 = np.expm1(gsXGB.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble - Stacked Regression and GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ref) http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: https://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor_files/stacking_cv_regressor_overview.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"source: https://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor/\")\n",
    "Image(url= \"https://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor_files/stacking_cv_regressor_overview.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   45.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   45.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:43:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:43:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "StackingRegressor(meta_regressor=XGBRegressor(base_score=0.5, booster='gbtree',\n",
      "                                              colsample_bylevel=1,\n",
      "                                              colsample_bynode=1,\n",
      "                                              colsample_bytree=1, gamma=0,\n",
      "                                              importance_type='gain',\n",
      "                                              learning_rate=0.1,\n",
      "                                              max_delta_step=0, max_depth=3,\n",
      "                                              min_child_weight=1, missing=None,\n",
      "                                              n_estimators=100, n_jobs=1,\n",
      "                                              nthread=None,\n",
      "                                              objective='reg:linear',\n",
      "                                              random_state=0, reg_alpha=0,\n",
      "                                              reg_lambda=1, sc...\n",
      "                                           seed=None, silent=None,\n",
      "                                           subsample=0.8, verbosity=1),\n",
      "                              SVR(C=0.1, cache_size=200, coef0=1.6, degree=2,\n",
      "                                  epsilon=0.03, gamma='auto', kernel='poly',\n",
      "                                  max_iter=-1, shrinking=True, tol=0.001,\n",
      "                                  verbose=False),\n",
      "                              KernelRidge(alpha=0.93, coef0=1.5, degree=3,\n",
      "                                          gamma=0.001, kernel='polynomial',\n",
      "                                          kernel_params=None)],\n",
      "                  store_train_meta_features=False,\n",
      "                  use_features_in_secondary=False, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "XGB = XGBRegressor()\n",
    "\n",
    "ELNET = ElasticNet(random_state = 1)\n",
    "LCV=Lasso(random_state = 1)\n",
    "SV = SVR()\n",
    "KR = KernelRidge()\n",
    "XG = XGBRegressor()\n",
    "stack = StackingRegressor(regressors = [ELNET,LCV,XG,SV,KR], meta_regressor = XGB)\n",
    "\n",
    "params = {  \n",
    "    # mlxtend : 0.17 이하 버전기준..\n",
    "    \n",
    "#               'meta-xgbregressor__n_estimators' : [740*2],#740\n",
    "#               'meta-xgbregressor__learning_rate': [0.01/2], #0.01\n",
    "#                'meta-xgbregressor__min_child_weight':[0],\n",
    "#               'meta-xgbregressor__gamma':[0.1],\n",
    "#               'meta-xgbregressor__max_depth': [2],\n",
    "#               'meta-xgbregressor__subsample':[0.65],\n",
    "#               'meta-xgbregressor__colsample_bytree':[0.4],\n",
    "#               'meta-xgbregressor__reg_alpha':[0],\n",
    "#               'meta-xgbregressor__reg_lambda':[1],\n",
    "    \n",
    "        # mlxtend : 0.17 버전 기준 : http://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/\n",
    "                'meta_regressor__n_estimators' : [740*2],#740\n",
    "              'meta_regressor__learning_rate': [0.01/2], #0.01\n",
    "               'meta_regressor__min_child_weight':[0],\n",
    "              'meta_regressor__gamma':[0.1],\n",
    "              'meta_regressor__max_depth': [2],\n",
    "              'meta_regressor__subsample':[0.65],\n",
    "              'meta_regressor__colsample_bytree':[0.4],\n",
    "              'meta_regressor__reg_alpha':[0],\n",
    "              'meta_regressor__reg_lambda':[1],\n",
    "\n",
    "              \n",
    "              'lasso__alpha':[0.00244736842105],\n",
    "              'elasticnet__alpha':[0.0276315789474],\n",
    "              'elasticnet__l1_ratio':[0.09],\n",
    "              'xgbregressor__min_child_weight':[0.2],\n",
    "              'xgbregressor__n_estimators' : [870],\n",
    "              'xgbregressor__learning_rate': [0.04],\n",
    "              'xgbregressor__gamma':[0],\n",
    "              'xgbregressor__max_depth': [3],\n",
    "              'xgbregressor__subsample':[0.8],\n",
    "              'xgbregressor__colsample_bytree':[0.7],\n",
    "    \n",
    "              'kernelridge__alpha':[0.93],\n",
    "              'kernelridge__coef0':[1.5],\n",
    "              'kernelridge__degree':[3],\n",
    "              'kernelridge__gamma':[0.001],\n",
    "              'kernelridge__kernel':['polynomial'],\n",
    "              'kernelridge__kernel_params':[None],\n",
    "              \n",
    "              'svr__coef0':[1.6],\n",
    "              'svr__kernel':['poly'],\n",
    "              'svr__epsilon':[0.03],\n",
    "              'svr__gamma': ['auto'],\n",
    "              'svr__degree': [2],\n",
    "              'svr__C':[0.1]\n",
    "        }\n",
    "\n",
    "grid = GridSearchCV(estimator = stack, param_grid=params,cv=kfold,refit=True, verbose=1,n_jobs=-1,scoring=\"neg_mean_squared_error\")\n",
    "grid.fit(X_train, y_df)\n",
    "grid_best = grid.best_estimator_\n",
    "print(grid_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:43:48] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:44:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:44:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:44:16] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:44:16] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:44:30] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:44:30] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:44:43] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:44:44] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:44:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:44:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:45:11] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Stacking mean score: 0.11520939471611036\n",
      "[09:45:11] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:45:25] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:45:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:45:40] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:45:40] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:45:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:45:55] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:46:08] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:46:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:46:24] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:46:25] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:46:39] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Stacking std: 0.008197710833127309\n"
     ]
    }
   ],
   "source": [
    "print(\"Stacking mean score:\", rmsle_cv(grid_best).mean())\n",
    "print(\"Stacking std:\", rmsle_cv(grid_best).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submission_st = np.expm1(grid.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble - Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submission_avg = (y_submission_6 + y_submission_2 + y_submission_st)/3\n",
    "\n",
    "#y_submission_avg = ( 5 * y_submission_6 + 2 * y_submission_2 + 1 * y_submission_st)/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([124050.41552813, 162062.44566583, 190340.67219535, ...,\n",
       "       163178.33501296, 117098.4432756 , 222331.02387479])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_submission_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame()\n",
    "my_submission['Id'] = test_id\n",
    "my_submission['SalePrice'] = y_submission_avg\n",
    "my_submission.to_csv('submission_trail_ver_0-1.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
